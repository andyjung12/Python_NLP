{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = \"He is the king . The king is royal . She is the royal queen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['He', 'is', 'the', 'king'],\n",
       " ['The', 'king', 'is', 'royal'],\n",
       " ['She', 'is', 'the', 'royal', 'queen']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_sentence = corpus_raw.split(\".\")\n",
    "sentences = []\n",
    "for sentence in raw_sentence:\n",
    "    sentences.append(sentence.strip().split())\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['He', 'is'],\n",
       " ['He', 'the'],\n",
       " ['is', 'He'],\n",
       " ['is', 'the'],\n",
       " ['is', 'king'],\n",
       " ['the', 'He'],\n",
       " ['the', 'is'],\n",
       " ['the', 'king'],\n",
       " ['king', 'is'],\n",
       " ['king', 'the'],\n",
       " ['The', 'king'],\n",
       " ['The', 'is'],\n",
       " ['king', 'The'],\n",
       " ['king', 'is'],\n",
       " ['king', 'royal'],\n",
       " ['is', 'The'],\n",
       " ['is', 'king'],\n",
       " ['is', 'royal'],\n",
       " ['royal', 'king'],\n",
       " ['royal', 'is'],\n",
       " ['She', 'is'],\n",
       " ['She', 'the'],\n",
       " ['is', 'She'],\n",
       " ['is', 'the'],\n",
       " ['is', 'royal'],\n",
       " ['the', 'She'],\n",
       " ['the', 'is'],\n",
       " ['the', 'royal'],\n",
       " ['the', 'queen'],\n",
       " ['royal', 'is'],\n",
       " ['royal', 'the'],\n",
       " ['royal', 'queen'],\n",
       " ['queen', 'the'],\n",
       " ['queen', 'royal']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "WINDOW_SIZE = 2\n",
    "\n",
    "for sentence in sentences:\n",
    "    for word_index, word in enumerate(sentence):\n",
    "        start_index = max(word_index - WINDOW_SIZE, 0)\n",
    "        end_index = min(word_index + WINDOW_SIZE + 1, len(sentence))\n",
    "        \n",
    "        for nb_word in sentence[start_index : word_index]:\n",
    "            data.append([word, nb_word])\n",
    "        \n",
    "        for nb_word in sentence[word_index + 1 : end_index]:\n",
    "            data.append([word, nb_word])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'She': 0, 'The': 1, 'He': 2, 'is': 3, 'king': 4, 'royal': 5, 'queen': 6, 'the': 7}\n",
      "{0: 'She', 1: 'The', 2: 'He', 3: 'is', 4: 'king', 5: 'royal', 6: 'queen', 7: 'the'}\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for word in corpus_raw.split():\n",
    "    if word != \".\":\n",
    "        words.append(word)\n",
    "words = set(words)\n",
    "\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "vocab_size = len(words)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word\n",
    "\n",
    "print(word2int)\n",
    "print(int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(word_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[word_index] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "for words in data:\n",
    "    x_train.append(to_one_hot(word2int[words[0]], vocab_size))\n",
    "    y_train.append(to_one_hot(word2int[words[1]], vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.asarray(x_train, dtype=np.float32)\n",
    "y_train = np.asarray(y_train, dtype=np.float32)\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, vocab_size=10, embedding_dim=5, optimizer='sgd', epochs=1000, learning_rate=0.01):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        if optimizer == 'adam':\n",
    "            self.optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "        else:\n",
    "            self.optimizer = tf.optimizers.SGD(learning_rate=learning_rate)\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # hidden layer 파라미터 설정\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim]))\n",
    "        self.b1 = tf.Variable(tf.random.normal([self.embedding_dim]))\n",
    "        \n",
    "        # output layer 파라미터 설정\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.embedding_dim, self.vocab_size]))\n",
    "        self.b2 = tf.Variable(tf.random.normal([self.vocab_size]))\n",
    "\n",
    "    def vectorized(self, word_index):\n",
    "        return (self.W1 + self.b1)[word_index]\n",
    "\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        for i in range(self.epochs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                hidden_layer = tf.add(tf.matmul(x_train, self.W1), self.b1)\n",
    "                output_layer = tf.add(tf.matmul(hidden_layer, self.W2), self.b2)\n",
    "\n",
    "                pred = tf.nn.softmax(output_layer)\n",
    "                loss = tf.reduce_mean(-tf.math.reduce_sum(y_train*tf.math.log(pred), axis=[1]))\n",
    "\n",
    "                grads = tape.gradient(loss, [self.W1, self.b1, self.W2, self.b2])\n",
    "                self.optimizer.apply_gradients(zip(grads, [self.W1, self.b1, self.W2, self.b2]))\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.3823414, shape=(), dtype=float32)\n",
      "tf.Tensor(1.4145879, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3840038, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3787315, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3769333, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3760853, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3756049, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3752997, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3750906, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3749387, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2Vec(vocab_size=vocab_size, embedding_dim=5, optimizer='SGD', epochs=10000, learning_rate=0.1)\n",
    "w2v.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.95338166, -0.27246356, -3.5666904 ,  0.04086742, -0.00802386],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.vectorized(word2int['queen']).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.7974787 , -1.1826966 , -0.8962433 , -0.41481996, -0.1611808 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.vectorized(word2int['king']).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = w2v.W1 + w2v.b1\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "\n",
    "model = TSNE(n_components=2, random_state=42) # 2차원으로 바꿈\n",
    "np.set_printoptions(suppress=True)\n",
    "vectors = model.fit_transform(vector)\n",
    "\n",
    "normalizer = preprocessing.Normalizer() # -1, 1 사이를 표현\n",
    "vectors = normalizer.fit_transform(vectors,'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen [0.94978195 0.31291252]\n",
      "royal [ 0.787192   -0.61670804]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(left=-1, right=1)\n",
    "ax.set_ylim(bottom=-1, top=1)\n",
    "for word in words:\n",
    "    print(word,vectors[word2int[word]])\n",
    "    ax.annotate(word, (vectors[word2int[word]][0], vectors[word2int[word]][1]))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_news = pd.read_csv('news.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>media</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>ngrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>연합인포맥스</td>\n",
       "      <td>내년 美국채수익률 5% 넘어서기 어려울 듯</td>\n",
       "      <td>2005년 10년만기 미국 국채수익률이  연방준비제도이사회(FRB)의 금리인상 지속...</td>\n",
       "      <td>만기/NNG,국채/NNG,수익률/NNG,fed/NNG,fed/NNG,금리/NNG,인...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>연합인포맥스</td>\n",
       "      <td>[뉴욕채권-마감] 10년만기 국채수익률 작년보다 낮은 수준서 마쳐</td>\n",
       "      <td>2년만기 국채가격 4년래 최악의 한해 보내     10년만기 미국  국채수익률이  ...</td>\n",
       "      <td>만기/NNG,국채/NNG,가격/NNG,최악/NNG,보내/VV,만기/NNG,국채/NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>연합인포맥스</td>\n",
       "      <td>[뉴욕환시] `내년초 달러-엔에 주력'..달러 對엔 하락</td>\n",
       "      <td>2004년 마지막 거래일인 31일  뉴욕환시에서 미국 달러화는 개장초 102엔 근처...</td>\n",
       "      <td>마지막/NNG,거래일/NNG,뉴욕/NNG,환시/NNG,달러/NNG,개장/NNG,근처...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>연합인포맥스</td>\n",
       "      <td>[31일 뉴욕금융시장 요약] 한산한 거래속 새해 준비</td>\n",
       "      <td>) 한해 마지막 날인 31일 뉴욕 주요 금융시장은 한산한 거래속에 새해를 준비하는 ...</td>\n",
       "      <td>마지막/NNG,뉴욕/NNG,금융시장/NNG,한산/NNG,거래/NNG,새해/NNG,준...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-01-02</td>\n",
       "      <td>연합인포맥스</td>\n",
       "      <td>美 증시 기술주 주도로 2년 연속 상승..'01년래 최고</td>\n",
       "      <td>지난해 뉴욕증시는 기술주 주도로 2년  연속연초 대비 상승하면서 대표지수들을 지난 ...</td>\n",
       "      <td>주도/NNG,연속/NNG,대비/NNG,상승/NNG,대표지수/NNG,최고/NNG,오르...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   media                                 title  \\\n",
       "0  2005-01-01  연합인포맥스               내년 美국채수익률 5% 넘어서기 어려울 듯   \n",
       "1  2005-01-01  연합인포맥스  [뉴욕채권-마감] 10년만기 국채수익률 작년보다 낮은 수준서 마쳐   \n",
       "2  2005-01-01  연합인포맥스       [뉴욕환시] `내년초 달러-엔에 주력'..달러 對엔 하락   \n",
       "3  2005-01-01  연합인포맥스         [31일 뉴욕금융시장 요약] 한산한 거래속 새해 준비   \n",
       "4  2005-01-02  연합인포맥스       美 증시 기술주 주도로 2년 연속 상승..'01년래 최고   \n",
       "\n",
       "                                             content  \\\n",
       "0  2005년 10년만기 미국 국채수익률이  연방준비제도이사회(FRB)의 금리인상 지속...   \n",
       "1  2년만기 국채가격 4년래 최악의 한해 보내     10년만기 미국  국채수익률이  ...   \n",
       "2  2004년 마지막 거래일인 31일  뉴욕환시에서 미국 달러화는 개장초 102엔 근처...   \n",
       "3  ) 한해 마지막 날인 31일 뉴욕 주요 금융시장은 한산한 거래속에 새해를 준비하는 ...   \n",
       "4  지난해 뉴욕증시는 기술주 주도로 2년  연속연초 대비 상승하면서 대표지수들을 지난 ...   \n",
       "\n",
       "                                              ngrams  \n",
       "0  만기/NNG,국채/NNG,수익률/NNG,fed/NNG,fed/NNG,금리/NNG,인...  \n",
       "1  만기/NNG,국채/NNG,가격/NNG,최악/NNG,보내/VV,만기/NNG,국채/NN...  \n",
       "2  마지막/NNG,거래일/NNG,뉴욕/NNG,환시/NNG,달러/NNG,개장/NNG,근처...  \n",
       "3  마지막/NNG,뉴욕/NNG,금융시장/NNG,한산/NNG,거래/NNG,새해/NNG,준...  \n",
       "4  주도/NNG,연속/NNG,대비/NNG,상승/NNG,대표지수/NNG,최고/NNG,오르...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261817, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2005년 10년만기 미국 국채수익률이  연방준비제도이사회(FRB)의 금리인상 지속에 따른 인플레이션 압력 완화로 연 5%를 넘어서기 어려울 것으로 전망됐다.    31일 씨티그룹 애널리스트들은 2005년에 미국의 인플레가 잘 제어될 것이라면서반면 내년 2년만기 국채수익률은 FRB의 지속적인 금리인상으로 연 4.00-4.50%  수준까지 상승하게 될 것이라고 예측했다.    씨티그룹은 단기 국채수익률이 상승세를 나타낼 것으로 보이는 반면 장기  국채수익률의 상승폭은 제한될 것으로 보여 수익률 곡선 평탄화가 가속화될 것이라고 덧붙였다.    씨티그룹은 내년 고용창출 호조가 가구당 수입증가를 견인할 것이라면서 고용시장 호전이 소비자지출을 떠받치게 될 것이라고 말했다'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_news['content'].apply(lambda x : x.split('.    ')).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [mecab.morphs(sent) for para in sentences for sent in para]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=sentences, \n",
    "                 size=100, \n",
    "                 alpha=0.025, \n",
    "                 window=5, \n",
    "                 min_count=2,\n",
    "                 sample=0.001,\n",
    "                 sg = 1, \n",
    "                 workers=4, \n",
    "                 iter=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
