{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPyMXdYHE0wn"
   },
   "source": [
    "# 감정분석(Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j61gv6QEE71q"
   },
   "source": [
    "# 사전기반 감정분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rVgDReTWWlDi",
    "outputId": "0c93c2d1-889d-4ed7-ac68-9324e3a6fc1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-15 14:42:21--  https://raw.githubusercontent.com/park1200656/KnuSentiLex/master/pos_pol_word.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 85007 (83K) [text/plain]\n",
      "Saving to: ‘pos_pol_word.txt’\n",
      "\n",
      "\r",
      "pos_pol_word.txt      0%[                    ]       0  --.-KB/s               \r",
      "pos_pol_word.txt    100%[===================>]  83.01K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-03-15 14:42:21 (5.19 MB/s) - ‘pos_pol_word.txt’ saved [85007/85007]\n",
      "\n",
      "--2021-03-15 14:42:21--  https://raw.githubusercontent.com/park1200656/KnuSentiLex/master/neg_pol_word.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 176542 (172K) [text/plain]\n",
      "Saving to: ‘neg_pol_word.txt’\n",
      "\n",
      "neg_pol_word.txt    100%[===================>] 172.40K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-03-15 14:42:22 (5.99 MB/s) - ‘neg_pol_word.txt’ saved [176542/176542]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#KnuSentiLex 다운로드\n",
    "!wget https://raw.githubusercontent.com/park1200656/KnuSentiLex/master/neg_pol_word.txt\n",
    "!wget https://raw.githubusercontent.com/park1200656/KnuSentiLex/master/pos_pol_word.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3o_a5Zoc7Qy",
    "outputId": "184571b4-12af-481c-cb68-5f9057548d4b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(-;', '(^^)', '(^-^)', '(^^*', '(^_^)', '(^o^)', '*^^*', '/^o^\\\\', ':(', \":'-(\"]\n",
      "['가난', '가난뱅이', '가난살이', '가난살이하다', '가난설음', '가난에', '가난에 쪼들려서', '가난하게', '가난하고', '가난하고 어렵다']\n"
     ]
    }
   ],
   "source": [
    "dct = {}\n",
    "with open ('pos_pol_word.txt', encoding = 'UTF8') as pos, open('neg_pol_word.txt', encoding = 'UTF8') as neg:\n",
    "    dct['pos'] = pos.read().split('\\n')[19:]\n",
    "    dct['neg'] = neg.read().split('\\n')[19:]\n",
    "print(dct['pos'][:10])\n",
    "print(dct['neg'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IzR_KFqQQ-pN"
   },
   "outputs": [],
   "source": [
    "txt = \"코로나19의 여파로 전 세계 교사들과 학생들 모두 혼란스러운 시기를 보내고 있습니다. 초유의 상황에서도, 학생들을 위해 최선을 다하려는 선생님들의 노력이 계속되고 있는데요. 어려움 속에서, 선생님들은 어떤 방법으로 사랑을 전하고 있을까요? 뉴스G에서 전해드립니다. [리포트] 중국 저장성의 한 산골 마을. 초등학교 교사 왕진량 씨는 지난 2월 말부터, 매일 새벽 다섯 시면 부지런히 집을 나섭니다. 온라인 수업을 받을 수 없는, 깊은 산골 마을에 살고 있는 학생들을 찾아가기 위해서인데요. 선생님이 하루 동안 이동하는 거리는 대략 30km정도. 차도 없이 도보로 네 개의 마을을 돌아다니며 학생들을 만납니다. 매일 이어지는 강행군이지만, 오로지 아이들의 학습이 중단되어서는 안 된다는 생각뿐입니다. 혹시 모를 사태에 대비해 학생들과의 접촉은 최대한 줄입니다. 숙제를 내주고, 검사 후에 모르는 문제를 알려주는 식으로 일대일 수업을 진행하고 있는데요. 아이들에게 배우는 즐거움이 얼마나 소중한 것인지 잘 알기에, 선생님은 이렇게라도 수업을 할 수 있다는 데서 행복을 느낍니다. 영국의 한 초등학교 교사인 젠 포울스 씨는 매일 아침, 무거운 짐을 앞 뒤, 양 옆으로 짊어지고 씩씩하게 발걸음을 옮깁니다. 코로나19로 학교가 문을 닫은 뒤, 형편이 어려운 학생들을 위해 매일 78인분의 점심 도시락을 배달하고 있는데요. 선생님이 재직 중인 초등학교는 전체 학생의 41퍼센트가 무상 급식 대상자이기 때문입니다. 도시락의 무게는 18kg, 걸어야 하는 거리는 8km에 달하지만 기다리는 학생들을 생각하며 지치지 않고 발걸음을 재촉합니다. 학생들은 창문을 통해서 반갑게 인사하기도 하고, 선생님이 볼 수 있게 감사 메시지를 붙여 놓기도 하는데요. 선생님이 정성껏 준비해 손수 배달한 사랑의 도시락. 봉쇄된 도시의 굶주린 아이들에게 소중한 한 끼 식사, 그 이상의 의미가 되고 있습니다. 미국 사우스다코타 주의 중학교 수학 교사인 크리스 와바 씨는, 커다란 화이트보드를 들고 학생의 집을 찾았습니다. 온라인 수업 후, 학생에게 이메일로 방정식 풀이법에 대한 질문을 받았기 때문인데요. 이메일로 답변해주는 것보다 직접 풀이 과정을 보여주는 게 낫다는 생각이었죠. 깜짝 놀란 학생을 마주한 채, 선생님은 현관문 앞에서 열정적으로 문제를 풀기 시작했습니다. 이 열정적인 강의는, 학생이 풀이법을 완벽히 이해할 때까지 이어졌는데요. 바이러스는 전 세계 교실에 혼란을 불러왔지만, 선생님들의 노력은 저마다의 방식으로 계속되고 있습니다. 어려움 속에서도 학생들을 위해 안간힘을 쓰고 있는 모든 선생님들에게, 응원과 박수를 함께 보냅니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ZTgyj8kJSCkG"
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment(txt, dct) :\n",
    "    pos = []\n",
    "    neg = []\n",
    "    \n",
    "    for p in dct['pos']:\n",
    "        if p in txt:\n",
    "            pos.append(p)\n",
    "#    for t in txt:\n",
    "#        if t in pos:\n",
    "#            pos.append(t)\n",
    "\n",
    "    for n in dct['neg']:\n",
    "        if n in txt:\n",
    "            neg.append(n)\n",
    "    return (len(pos)/(len(pos)+len(neg)), pos), (len(neg)/(len(pos)+len(neg)), neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVRRF_5SSiNf",
    "outputId": "326f46b1-a35c-4a27-b4f7-b7397cc00200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6470588235294118, ['낫다', '대상', '사랑을', '씩씩하게', '열정', '운', '잘', '정', '정성껏', '함께', '감사', '복을', '부지런', '부지런히', '사랑', '사랑의', '소중한', '즐거움', '즐거움이', '행복', '행복을', ''])\n",
      "(0.35294117647058826, ['방정', '어려운', '어려움', '해', '혼란스러운', '화', '굶주린', '모르는', '바이러스', '우는', '짐', ''])\n"
     ]
    }
   ],
   "source": [
    "pos, neg = analyze_sentiment(txt, dct)\n",
    "print(pos)\n",
    "print(neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anKdEE0eEk8z"
   },
   "source": [
    "# 나이브 베이즈 분류기 활용 감정분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTVyt3BRKY7M"
   },
   "source": [
    "## 간단 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "IoLvW8daCLbo"
   },
   "outputs": [],
   "source": [
    "text = [(\"I love you\", \"P\"),\n",
    "        (\"love happy weekend\", \"P\"),\n",
    "        (\"bore work job\", \"N\"),\n",
    "        (\"I hate you\", \"N\"),\n",
    "        (\"bore weekend\", \"N\"),\n",
    "        (\"happy together\", \"P\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'I': [1, 1],\n",
       "             'love': [2, 0],\n",
       "             'you': [1, 1],\n",
       "             'happy': [2, 0],\n",
       "             'weekend': [1, 1],\n",
       "             'bore': [0, 2],\n",
       "             'work': [0, 1],\n",
       "             'job': [0, 1],\n",
       "             'hate': [0, 1],\n",
       "             'together': [1, 0]})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 범주에 속하는 토큰수 세기 0(긍정), 1(부정))\n",
    "doccnt0 = 0\n",
    "doccnt1 = 0\n",
    "\n",
    "# 토큰별로 문서내 빈도수 카운팅\n",
    "wordfreq = defaultdict(lambda : [0, 0])\n",
    "for doc, target in text:\n",
    "    label = 0 if target == 'P' else 1\n",
    "    words = doc.split()\n",
    "    for word in words :\n",
    "        wordfreq[word][label] += 1\n",
    "          \n",
    "for key, (cnt0, cnt1) in wordfreq.items() :\n",
    "    doccnt0 += cnt0\n",
    "    doccnt1 += cnt1\n",
    "\n",
    "wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kue74JQriF2P",
    "outputId": "b04cc948-4fc2-4bf9-bc0e-1ffb58adcdb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'I': [1, 1],\n",
       "             'love': [2, 0],\n",
       "             'you': [1, 1],\n",
       "             'happy': [2, 0],\n",
       "             'weekend': [1, 1],\n",
       "             'bore': [0, 2],\n",
       "             'work': [0, 1],\n",
       "             'job': [0, 1],\n",
       "             'hate': [0, 1],\n",
       "             'together': [1, 0]})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 범주에 속하는 토큰수 세기 0(긍정), 1(부정))\n",
    "doccnt0 = 0\n",
    "doccnt1 = 0\n",
    "\n",
    "# 토큰별로 문서내 빈도수 카운팅\n",
    "wordfreq = defaultdict(lambda : [0, 0])\n",
    "for doc, target in text:\n",
    "    label = 0 if target == 'P' else 1\n",
    "    words = doc.split()\n",
    "    for word in words :\n",
    "        wordfreq[word][label] += 1\n",
    "          \n",
    "for key, (cnt0, cnt1) in wordfreq.items() :\n",
    "    doccnt0 += cnt0\n",
    "    doccnt1 += cnt1\n",
    "\n",
    "wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_F5iowywxCT",
    "outputId": "5128b6a6-946b-490f-d1e5-cc4e6fb96cf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doccnt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3DJjU95vJN8",
    "outputId": "7d7fce8a-b69b-431e-a75a-f057f14d8477"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'I': [0.16666666666666666, 0.16666666666666666],\n",
       "             'love': [0.2777777777777778, 0.05555555555555555],\n",
       "             'you': [0.16666666666666666, 0.16666666666666666],\n",
       "             'happy': [0.2777777777777778, 0.05555555555555555],\n",
       "             'weekend': [0.16666666666666666, 0.16666666666666666],\n",
       "             'bore': [0.05555555555555555, 0.2777777777777778],\n",
       "             'work': [0.05555555555555555, 0.16666666666666666],\n",
       "             'job': [0.05555555555555555, 0.16666666666666666],\n",
       "             'hate': [0.05555555555555555, 0.16666666666666666],\n",
       "             'together': [0.16666666666666666, 0.05555555555555555]})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 0.5\n",
    "\n",
    "wordprobs = defaultdict(lambda : [0, 0])\n",
    "for key, (cnt0, cnt1) in wordfreq.items() :\n",
    "    wordprobs[key][0] = (cnt0 + k) / (doccnt0 + 2*k)\n",
    "    wordprobs[key][1] = (cnt1 + k) / (doccnt1 + 2*k)\n",
    "\n",
    "wordprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ffr9IjijvQ7t",
    "outputId": "8b7c5495-784a-42cc-97e9-ae56bf73d45b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy weekend\n",
      "긍정확률 : 83.33333333333333%\n",
      "부정확률 : 16.666666666666664%\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "doc = \"happy weekend\"\n",
    "\n",
    "tokens = doc.split()\n",
    "\n",
    "# 초기값은 모두 0으로 처리\n",
    "log_prob1 = log_prob0 = 0.0\n",
    "\n",
    "# 모든 단어에 대해 반복\n",
    "for word, (prob0, prob1) in wordprobs.items():\n",
    "    if word in tokens:\n",
    "        log_prob1 += math.log(prob1)\n",
    "        log_prob0 += math.log(prob0)\n",
    "\n",
    "log_prob0 += math.log(doccnt0/(doccnt0 + doccnt1))    \n",
    "log_prob1 += math.log(doccnt1/(doccnt0 + doccnt1))\n",
    "  \n",
    "prob0 = math.exp(log_prob0)\n",
    "prob1 = math.exp(log_prob1)\n",
    "\n",
    "print(doc)\n",
    "print(\"긍정확률 : {}%\".format(prob0 / (prob0 + prob1)*100))\n",
    "print(\"부정확률 : {}%\".format(prob1 / (prob0 + prob1)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "b8MGr_sZibyp"
   },
   "outputs": [],
   "source": [
    "X_train = [t[0] for t in text]\n",
    "Y_train = [t[1] for t in text]\n",
    "X_train\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "clf = MultinomialNB().fit(X_train_counts, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x8zJ50ooDOTq",
    "outputId": "bb92d9b3-8ebc-4f32-a796-61c36311a025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P']\n",
      "[[0.25 0.75]]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(count_vect.transform([\"happy weekend\"])))\n",
    "print(clf.predict_proba(count_vect.transform([\"happy weekend\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFINscB8FS7Q"
   },
   "source": [
    "## 네이버 영화 리뷰 감정분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 영화 리뷰 다운로드\n",
    "!wget https://github.com/e9t/nsmc/raw/master/ratings.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "gQIY8T7NFe0n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id                                           document  label\n",
      "0        8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
      "1        8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
      "2        4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
      "3        9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
      "4       10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1\n",
      "...          ...                                                ...    ...\n",
      "199995   8963373                                     포켓 몬스터 짜가 ㅡㅡ;;      0\n",
      "199996   3302770                                              쓰.레.기      0\n",
      "199997   5458175                  완전 사이코영화. 마지막은 더욱더 이 영화의질을 떨어트린다.      0\n",
      "199998   6908648                왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ      0\n",
      "199999   8548411                                    포풍저그가나가신다영차영차영차      0\n",
      "\n",
      "[199992 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"./ratings.txt\",sep='\\t').dropna()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "gJnjV-KQFnga"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['document'], df['label'], random_state=42)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Qecu_93Hwu7",
    "outputId": "ce1e2f23-eb8e-4350-d288-c3bd324fc09e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189879                                       모피너무많이입고나와 불쾌함\n",
       "87482                        결말이 ㅠ ㅠ 멜비푸포 완전멋져요 ㅠ이제서야알다니 ㅠㅠ\n",
       "36369                                 타케모토 노보루 감독, 최고의 걸작이다\n",
       "82006       진짜 인생 애니메이션 어릴땐 상상력자극으로 재밌고 커서는 우정간의 감동을 보며 재밌고\n",
       "89163     몇몇 맞지않는 전개도 있지만 이 정도면 훌륭하다. 우리 이순신장군님 영화도 이렇게 ...\n",
       "79195     번개치고 귀신튀어나오는 성궤나 불사의 영약 성배나 고대 중남미문명 외계인 개입설이나...\n",
       "94218     우리나라 몇 안되는 진행 몰입도 좋은 영화중 하나. 2편이 기대되는데 안나올듯 하다...\n",
       "124006    네러티브는 하나도 없는 예쁜 음악 동영상. 취미로 영화만드는 것은 이걸로 끝나길 ....\n",
       "27053                              나이를 먹을수록 가슴깊이 느낄 수 있는 영화\n",
       "82529                         진ㅉ ㅏ잼잇따!!!!!!!후끈후끈화끈화끈 액숀!!!!\n",
       "Name: document, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPzdSWaRFvV1",
    "outputId": "01d8b114-6421-4795-aa30-3f4e0bfc54a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.828213128525141"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new_counts = count_vect.transform(X_test)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_counts)\n",
    "np.mean(predicted == y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "11 Prac. Sentiment Analysis.ipynb의 사본",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
